{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oKXJO-Akpma"
      },
      "source": [
        "# Model Inference\n",
        "\n",
        "Nama: Elia Oktaviani\n",
        "\n",
        "Pada bagian ini akan dijalankan uji model dengan data yang belum pernah ditemuui sebelumnya. Data yang diuji berupa teks dengan kata-kata positif dan bertujuan untuk menghasilkan suatu sentiment positif."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMAVtXOElKoc"
      },
      "source": [
        "#Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-hby2LYV8gH",
        "outputId": "c86c02d3-a970-4cc7-cb7b-a73042cc73b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# General use\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Text-related\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# NN-related\n",
        "from tensorflow.keras.saving import load_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1QBvvORlNWK"
      },
      "source": [
        "# Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "OpqNkM5iXafF",
        "outputId": "a249a687-a8cf-4c61-ce53-9689b4fa1cf9"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "model = load_model('model_improved')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npYftGBYlY70"
      },
      "source": [
        "#Model Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE_cw0jYlbEh"
      },
      "source": [
        "Pada bagian ini telah dibuat rangkaian fungsi yang sudah disusun sedemikian rupa sehingga raw data yang dimasukan dapat di proses jadi hasil prediksi jenis sentimen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iAv0Tl05Uyxi"
      },
      "outputs": [],
      "source": [
        "def predict_tfidf(text):\n",
        "\n",
        "    # Placeholder for text preprocessing (cleaning, lemmatization, and removal of stopwords)\n",
        "    def preprocess_text(text):\n",
        "        # Expand contractions\n",
        "        text = expand_contractions(text)\n",
        "\n",
        "        # Case folding\n",
        "        text = text.lower()\n",
        "\n",
        "        # Mention removal\n",
        "        text = re.sub(\"@[A-Za-z0-9_]+\", \" \", text)\n",
        "        text = re.sub(\"@ [A-Za-z0-9_]+\", \" \", text)\n",
        "\n",
        "        # Hashtags removal\n",
        "        text = re.sub(\"#[A-Za-z0-9_]+\", \" \", text)\n",
        "\n",
        "        # Newline removal (\\n)\n",
        "        text = re.sub(r\"\\\\n\", \" \",text)\n",
        "\n",
        "        # Whitespace removal\n",
        "        text = text.strip()\n",
        "\n",
        "        # URL removal\n",
        "        text = re.sub(r\"http\\S+\", \" \", text)\n",
        "        text = re.sub(r\"www.\\S+\", \" \", text)\n",
        "        text = re.sub(r\"twitch.tv\\S+\", \" \", text)\n",
        "        text = re.sub(r\"twitch tv\\S+\", \" \", text)\n",
        "        text = re.sub(r\"pic.twitter.com\\S+\", \" \", text)\n",
        "        text = re.sub(r\"dlvr.it\\S+\", \" \", text)\n",
        "        text = re.sub(r\"dfr.it / RMTrgF\", \" \", text)\n",
        "        text = re.sub(r\"dlvr.it\\S+\", \" \", text)\n",
        "        text = re.sub(r\"dlvr.it \\S+\", \" \", text)\n",
        "\n",
        "\n",
        "        # Non-letter removal (such as emoticon, symbol (like μ, $, 兀), etc\n",
        "        text = re.sub(\"[^A-Za-z\\s']\", \" \", text)\n",
        "\n",
        "        # Tokenization\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Stopwords removal\n",
        "        tokens = [word for word in tokens if word not in stpwds_en ]\n",
        "        tokens = [word for word in tokens if word not in additional_stopwords]\n",
        "\n",
        "        # Lemmatizing\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "        # Combining Tokens\n",
        "        text = ' '.join(tokens)\n",
        "        return text\n",
        "\n",
        "    # Preprocess the input text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # Vectorize the preprocessed text\n",
        "    x = vectorizer.transform([preprocessed_text])\n",
        "\n",
        "    # Predict the probabilities\n",
        "    predictions_proba = model.predict(x.toarray())\n",
        "\n",
        "    # Assuming a multi-class classification with one-hot encoded labels\n",
        "    emotions = {0: 'Irrelevant', 1: 'Negative', 2: 'Neutral', 3:'Positive'}\n",
        "\n",
        "    # Get class with maximum probability\n",
        "    prediction = np.argmax(predictions_proba, axis=-1)\n",
        "\n",
        "    if prediction == 0:\n",
        "        print(\"Sentiment: Irrelevant\")\n",
        "    elif prediction == 1:\n",
        "        print(\"Sentiment: Negative\")\n",
        "    elif prediction == 2:\n",
        "        print(\"Sentiment: Neutral\")\n",
        "    elif prediction == 3:\n",
        "        print(\"Sentiment: Positive\")\n",
        "    else:\n",
        "        print(\"Unknown sentiment\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_CzssqiVzjP"
      },
      "outputs": [],
      "source": [
        "\n",
        "txt = 'This game is very interesting, and i do super love it'\n",
        "predict_tfidf(txt)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
